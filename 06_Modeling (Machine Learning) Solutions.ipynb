{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Create an additional evaluation function that evaluates the __root mean squared error (RMSE)__ instead of the mean absolute error (MAE). Hint: Look again in the scikit-learn documentation for help.\n",
    "\n",
    "2. Compare the resulting RMSE with the MAE. How do you interpret the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "melbourne_file_path = 'https://raw.githubusercontent.com/vhaus63/ids_data/refs/heads/main/melb_data.csv'\n",
    "melbourne_data = pd.read_csv(melbourne_file_path)\n",
    "melbourne_data.dropna(axis=0, inplace=True)\n",
    "\n",
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea',\n",
    "                        'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "\n",
    "X = melbourne_data[melbourne_features]\n",
    "y = melbourne_data['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Define\n",
    "melbourne_model = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "# Fit\n",
    "melbourne_model.fit(X, y)\n",
    "\n",
    "predicted_home_prices = melbourne_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE of our model is: 434.71594577146544\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y, predicted_home_prices)\n",
    "\n",
    "print(\"The MAE of our model is: {}\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of our model is: 6616.258377610625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error \n",
    "\n",
    "rmse = root_mean_squared_error(y, predicted_home_prices)\n",
    "\n",
    "print(\"The RMSE of our model is: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is much larger. This is due to the fact that the RMSE takes into account outliers much stronger than the MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "1. Repeat the previous model training using a linear regression instead of the decision tree. Hint: Check out the [scikit-learn documentation](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html), where the  regression model is decribed and code examples are given.\n",
    "\n",
    "2. Create an additional evaluation function that evaluates the root mean squared error (rmse) instead of the mean absolute error. Hint: Look again in the scikit-learn documentation for help.\n",
    "\n",
    "3. Compare the resulting rmse with the mae. How do you interpret the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of our model is: 535669.304157012\n",
      "The MAE of our model is: 301277.67455197935\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Split the data\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "\n",
    "# Build the model...\n",
    "melbourne_linreg_model = LinearRegression()\n",
    "melbourne_linreg_model.fit(train_X, train_y)\n",
    "\n",
    "# Evaluate the performance\n",
    "val_predictions = melbourne_linreg_model.predict(val_X)\n",
    "val_rmse = root_mean_squared_error(val_y, val_predictions)\n",
    "val_mae = mean_absolute_error(val_y, val_predictions)\n",
    "\n",
    "print(\"The RMSE of our model is: {}\".format(val_rmse))\n",
    "print(\"The MAE of our model is: {}\".format(val_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE treats all errors equally, giving a linear penalty to each difference.\n",
    "RMSE penalizes larger errors more heavily because it squares the errors. This means RMSE is more sensitive to outliers or large prediction errors.\n",
    "\n",
    "Looking at the MAE, we can also observe that the linear regression performs worse than the decision tree. Thus, the housing data likely contains some non-linearities that the regression model cannot handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "1. If we have better information on the content of the categorical variables and time permits, we would not simply use the same encoder for all columns. Look at all columns of type \"object\" of the full data set (e.g. at the beginning of the notebook) and think about the adequate type of encoding (label or one-hot) for each of these columns. You find a description of the variables [here](https://www.kaggle.com/datasets/dansbecker/melbourne-housing-snapshot).\n",
    "2. (Homework, optional): Retrain the model using your own, more nuanced encoding approach and try whether you can obtain a better predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "__Method:__ S - property sold; SP - property sold prior; PI - property passed in; PN - sold prior not disclosed; SN - sold not disclosed; NB - no bid; VB - vendor bid; W - withdrawn prior to auction; SA - sold after auction; SS - sold after auction price not disclosed. N/A - price or highest bid not available. __The variable has no natural ordering --> use one-hot encoder__\n",
    "\n",
    "__Type:__ br - bedroom(s); h - house,cottage,villa, semi,terrace; u - unit, duplex; t - townhouse; dev site - development site; o res - other residential. __The variable has no natural ordering --> use one-hot encoder__\n",
    "\n",
    "__SellerG:__ Real Estate Agent __The variable has no natural ordering --> use one-hot encoder__\n",
    "\n",
    "__Date:__ Date sold __This is time-series data. It should be converted into a numerical variable (e.g by using [`pandas.to_datetime()`](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html)__).\n",
    "\n",
    "__Regionname:__ General Region (West, North West, North, North east â€¦etc)__The variable has no natural ordering --> use one-hot encoder__\n",
    "\n",
    "Propertycount: Number of properties that exist in the suburb.\n",
    "\n",
    "Bedroom2 : Scraped # of Bedrooms (from different source)\n",
    "\n",
    "Bathroom: Number of Bathrooms\n",
    "\n",
    "Car: Number of carspots\n",
    "\n",
    "Landsize: Land Size\n",
    "\n",
    "BuildingArea: Building Size\n",
    "\n",
    "__CouncilArea:__ Governing council for the area __The variable has no natural ordering --> use one-hot encoder__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "We can train our model using the different datasets and feature engineering techniques to evaluate their impact on the model performance.\n",
    "Print the score for the different methods we just introduced and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this code to the original lecture notebook in order to execute it - it will not work here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" print(\"Raw (but imputed) Features: {}\".\n",
    "      format(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid)))\n",
    "print(\"Normalized Features: {}\".\n",
    "      format(score_dataset(train_X_normalized, val_X_normalized, y_train, y_valid)))\n",
    "print(\"Standardized Features: {}\".\n",
    "      format(score_dataset(train_X_standardized, val_X_standardized, y_train, y_valid)))\n",
    "print(\"Log Landsize: {}\".format(score_dataset(train_X_logGains, val_X_logGains, y_train, y_valid))) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the MAE is, the better. So in this case, transforming the Landsize to a logarithmic scale (only one column!) has the biggest effect on the performance. It should be noted that in general it is recommended to try different combinations of the possible transformations, not only one each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "Use standardized numeric features like in the previous section and combine it with OneHot Encoding and Label Encoding for training the model.\n",
    "\n",
    "The final results should have two cases: \n",
    "\n",
    "- One-hot encoded categorical + standardized numeric\n",
    "- Label encoded categorical + standardized numeric\n",
    "\n",
    "For each of the considered categorical columns, does it make more sense to use label encoding or one-hot encoding from a data perspective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this code to the original lecture notebook in order to execute it - it will not work here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" num_X_train = X_train.drop(low_cardinality_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(low_cardinality_cols, axis=1)\n",
    "\n",
    "# standardize numerical columns\n",
    "scaler = StandardScaler()\n",
    "num_train_X_standardized = pd.DataFrame(scaler.fit_transform(num_X_train), \n",
    "                                        columns=num_X_train.columns, index=num_X_train.index)\n",
    "num_val_X_standardized = pd.DataFrame(scaler.transform(num_X_valid), \n",
    "                                      columns=num_X_valid.columns, index=num_X_valid.index)\n",
    "\n",
    "\n",
    "# One-hot encoding\n",
    "one_hot_X_train = pd.concat([num_train_X_standardized, one_hot_cols_train], axis=1)\n",
    "one_hot_X_valid = pd.concat([num_val_X_standardized, one_hot_cols_valid], axis=1)\n",
    "\n",
    "# Evaluate performance\n",
    "one_hot_encoding = score_dataset(one_hot_X_train.to_numpy(), one_hot_X_valid.to_numpy(), y_train, y_valid)\n",
    "print(\"MAE using One-hot Encoding and Standardization: {}\".format(one_hot_encoding))\n",
    "\n",
    "\n",
    "# Label encoding\n",
    "label_X_train_cat = pd.DataFrame(columns=low_cardinality_cols)\n",
    "label_X_valid_cat = pd.DataFrame(columns=low_cardinality_cols)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for col in low_cardinality_cols:\n",
    "    label_X_train_cat[col] = label_encoder.fit_transform(X_train[col])\n",
    "    label_X_valid_cat[col] = label_encoder.transform(X_valid[col])\n",
    "\n",
    "# align indices\n",
    "label_X_train_cat.index = num_train_X_standardized.index\n",
    "label_X_valid_cat.index = num_val_X_standardized.index\n",
    "\n",
    "label_X_train = pd.concat([num_train_X_standardized, label_X_train_cat], axis=1)\n",
    "label_X_valid = pd.concat([num_val_X_standardized, label_X_valid_cat], axis=1)\n",
    "\n",
    "# Evaluate performance\n",
    "label_encoding = score_dataset(label_X_train, label_X_valid, y_train, y_valid)\n",
    "print(\"MAE using Label Encoding and Standardization: {}\".format(mae_label_encoding)) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three columns should be encoded with a One-hot Encoder since their categories don't have an explicit ordering. This is also reflected in the MAE of both the model with the standardized and the non-standardized numerical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "Extend the pipeline by including the normalization step for the numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this code to the original lecture notebook in order to execute it - it will not work here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Preprocessing numerical columns\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Imputation\n",
    "    ('scaler', MinMaxScaler())                 # Normalization\n",
    "])\n",
    "\n",
    "# Preprocessing categorical columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Imputation\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encoding\n",
    "])\n",
    "\n",
    "# Bundle both preprocessors\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_cols),  # Apply numerical pipeline\n",
    "    ('cat', categorical_transformer, low_cardinality_cols)  # Apply categorical pipeline\n",
    "])\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "complete_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Preprocess the raw training data and fit the model\n",
    "complete_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocess the raw validation data and make predictions\n",
    "preds = complete_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print(\"MAE using the complete pipeline: {}\".format(score)) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6\n",
    "Compute the adjusted R^2 Score. Explain the difference to the previously computed R^2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this code to the original lecture notebook in order to execute it - it will not work here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" adj_r2 = 1-(1-r2)*(len(y_test)-1)/(len(y_test)-X.shape[1]-1)\n",
    "adj_r2 \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjusted R^2 is always lower or equal to the R^2, since it additionally accounts for the number of predictors and adjusts for the possibility of overfitting. Since the difference here is not very big, the amount of predictors has not significantly harmed the predictive power of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7\n",
    "\n",
    "Create a linear regression model that predicts y again, but only using `Time on App` and `Length of Membership`. Compare the resulting regression report to the one above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this code to the original lecture notebook in order to execute it - it will not work here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" X_two_features = X[['Time on App', 'Length of Membership']]\n",
    "X_two_features = sm.add_constant(X_two_features)\n",
    "\n",
    "model = sm.OLS(y, X_two_features).fit()\n",
    "\n",
    "print(model.summary()) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Report 1 has a higher R^2 (0.984 vs. 0.881), indicating better explanatory power.\n",
    "- Report 1 indicates that `Time on Website` is insignificant (p=0.326). This observation favors Model 2 for simplicity reasons. It uses fewer predictors while maintaining a reasonably high R^2.\n",
    "- Report 1 has lower AIC (3724.0) and BIC (3745.0), indicating a better fit, but these values are influenced by the additional predictors.\n",
    "- Coefficients for Time on App and Length of Membership are very similar in both models, suggesting these variables are consistently important.\n",
    "- Report 1 notes a high condition number (2.64e+03), indicating potential multicollinearity (which is also described in the Notes). This may justify removing predictors (like `Time on Website`) in Report 2.\n",
    "\n",
    "In general, if the goal is solely model fit, Report 1 indicates better results. If however interpretability and simplicity are desired, the second model might be the better choice, since it removes insignificant variables and avoids potential multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8\n",
    "Calculate by hand:\n",
    "- the support of the item `milk`.\n",
    "- the confidence for the rule `milk` $\\rightarrow$ `bread`\n",
    "- the lift for the rule `milk` $\\rightarrow$ `bread`\n",
    "\n",
    "What does this tell you about the relationship of purchases between bread and milk?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Support(Milk) = $\\frac{3}{5}=0.6$. 60% of purchases contain Milk.\n",
    "- Support(Milk and Bread) = $\\frac{2}{5},$ so Confidence(Milk $\\rightarrow$ Bread) = $\\frac{0.4}{0.6}\\approx 0.6667$. This means that in 66.67% of the cases where Milk was bought, Bread is also purchased.\n",
    "- Lift(Milk $\\rightarrow$ Bread) = $\\frac{0.4}{0.6\\cdot 0.8}\\approx 0.8333$. Since the lift is smaller than one, there is a negative association between Milk and Bread. So they are bought less often together (as a set) than would be expected if they were independent. When Milk is purchased, Bread is less likely to be purchased in the same transaction than random chance would suggest.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
